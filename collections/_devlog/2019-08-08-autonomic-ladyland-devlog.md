---
title: On a sunny day I look up at the sky and watch the clouds
date: 2019-08-08T09:05:00+00:00
project: false
layout: post
video1: 354367068
soundcloud: 667217351
categories:
  - code
  - projects
  - technology
  - VR
tags:
  - projects
description: Synthesising biological signals with VR and audio
tech:
 - Oculus Rift
 - C#
 - Unity
 - Chuck
 - Arduino

---

<div class="img_row">
	<img class="col three" src="{{ site.baseurl }}/images/autonomicladyland/ALSky.jpg" alt="" title="Procedural clouds"/>
</div>
<div class="col three caption">
	Procedural Sky
</div>

**30.07.19**

_Outline_

Autonomic Ladyland is an artwork fusing Virtual Reality, bio-signal data and real-time sound synthesis to explore the relationship of the body to our natural world. In this work the body directly affects the experienced environment as the bio-signals generated by the human autonomic system cause changes in the surrounding environment. The subject experiences being the centre of the universe which swirls around her at the behest of her vital signals. Her life force is projected out into the universe.

<div class="img_row">
	<img class="col two left" src="{{ site.baseurl }}/images/autonomicladyland/IMG-1826.JPG" alt="" title="Sky Dome"/>
</div>
<div class="caption_row">
    <div class="col two left caption">Sky Dome</div>
</div>

The subject will be connected to various sensors before entering VR:
-   EEG (brain-wave sensor)
-   ECG (heart function sensor)
-   EMG (muscle sensor)
-   EDA (electrodermal activity)
-   Stretch sensor (chest inflation)

The subject will also receive data in the form of sound and light and also haptic excitement of the vagus nerve through the ear.

Each of the bio-signals will be mapped to an aspect of the system - possibly mediated by a deep learning neural network - so that as the subject's physiological state alters the perceived environment will also be modified. The nature of the relationship between the captured bio-data and the experienced environment will be explored as the piece develops.
 
 Sensory input will be threefold:
 - Oculus Rift VR (visual)
 - Noise cancelling headphones (auditory)
 - Haptic motor (touch)

<div class="img_row">
	<img class="col two left" src="{{ site.baseurl }}/images/autonomicladyland/IMG-1825.JPG" alt="" title="Rig"/>
</div>
<div class="caption_row">
    <div class="col two left caption">Rig</div>
</div>

**Hardware**

- Oculus Rift VR headset
- Senheiser Ambeo 3D audio headset
- Bitalino 
- Arduino Feather WiFi

**Software**

- Unity
- Chuck
- Arduino

- - -

**01.08.19**

_Set up the Oculus Rift in Unity._ 

I import the latest Oculus Integration Kit (1.39).. and.. no hands or controllers appear in the scene. It appears this version is broken. I downgrade to v1.38 and we are in business.

I purchase [Overcloud](https://assetstore.unity.com/publishers/25376){:target="_blank"} plugin for Unity. It looks like it should be good for creating the dynamic volumetric clouds I want but there are some strange artifacts appearing in VR. In vain I search for a fix. Admitting defeat I now purchase [WeatherMaker](https://assetstore.unity.com/packages/tools/particles-effects/weather-maker-unity-weather-system-sky-water-volumetric-clouds-a-60955?aid=1011lGnL&utm_source=aff){:target="_blank"}. Same problem: nasty artifacts in VR. More tweaking and I find the answer - neither of these plugins support multi-pass rendering for the Stereo Rendering Mode (Edit -> Project Settings -> Player -> XR Settings -> Stereo Rendering Mode). 

{% include vimeoplayer.html id=page.video1 %}
<div class="caption_row">
    <div class="col three center caption">Volumetric Clouds Test</div>
</div>

- - -

**05.08.19**

_Set up OSC in Unity to send and receive data._

To test my code I send OSC data from a small python script on my Mac across the local network. I have to open a port in the Windows firewall before I can receive the message.

Here is the C# script:

```csharp
using System.Collections;
using UnityEngine;

/// <summary>
/// This class uses https://github.com/thomasfredericks/UnityOSC
/// </summary>
public class PythonOSC : MonoBehaviour
{

    [SerializeField]
    private OSC _osc;
    
    void Start()
    {
        _osc.SetAddressHandler("/hello", OnHello); //simple 'hello world' test path
        _osc.SetAllMessageHandler(OnAnyMessage);        //handle any message
        StartCoroutine(DelaySendMessage());     //Test sending a message
    }

    /// <summary>
    /// Will send an OSC message to address "/hello" with 
    /// a single float parameter after 2 seconds
    /// </summary>
    /// <returns></returns>
    private IEnumerator DelaySendMessage()
    {
        yield return new WaitForSeconds(2);
        var msg = new OscMessage {address = "/hello"};
        msg.values.Add(0.1f);
        _osc.Send(msg);
    }
    
    /// <summary>
    /// Will handle all OSC messages arriving on the incoming port 
    /// </summary>
    /// <param name="oscm"></param>
    private void OnAnyMessage(OscMessage oscm)
    {
        Debug.Log("any message received");
    }

    /// <summary>
    /// Handler for the "/hello" address
    /// Parse the float from the message
    /// </summary>
    /// <param name="oscm"></param>
    private void OnHello(OscMessage oscm)
    {
        var str = oscm.GetFloat(0);
        Debug.Log($"Said Hello {str}");
    }

}
```

And the python script:

```python
"""Small example OSC client
This program sends 10 random values between 0.0 and 1.0 to the /filter address,
waiting for 1 seconds between each value.
"""
import argparse
import random
import time

from pythonosc import osc_message_builder
from pythonosc import udp_client


if __name__ == "__main__":
  parser = argparse.ArgumentParser()
  parser.add_argument("--ip", default="localhost",
      help="The ip of the OSC server")
  parser.add_argument("--port", type=int, default=3001,
      help="The port the OSC server is listening on")
  args = parser.parse_args()

  client = udp_client.SimpleUDPClient(args.ip, args.port)

  for x in range(10):
    val = random.random();
    print(args.ip, args.port,val);
    client.send_message("/hello", val)
    time.sleep(1)
```

I also create a receiver in ChucK to handle the value sent from Unity:

```javascript
//osc.ck

// create our OSC receiver
OscRecv oscin;

3001 => oscin.port;

// start listening (launch thread)
oscin.listen();

// ..or listen to all messages
//oscin.listenAll();

// create an address in the receiver 
// and store it in a new variable.
oscin.event("/hello,f") @=> OscEvent HelloEvent; 

while ( true ){
    
    HelloEvent => now;
    
    while ( HelloEvent.nextMsg() != 0 ){
        HelloEvent.getFloat() => eventBus.hrmEvent.val;

        eventBus.hrmEvent.broadcast();
    }
    
}
```

This completes the testing for OSC messaging to and from Unity. I'll deal with sending OSC over wifi from the Arduino Feather later..

- - -

**07.08.19**

_Set up ChucK architecture_

More code today. This time I'm setting up the core architecture of my ChucK layer. This will provide the necessary flexibility to allow me to receive OSC messages and use the data to change parameters of my ChucK instruments using the built-in event system.

First my directory hierarchy:

<div class="img_row">
	<img class="col one left" src="{{ site.baseurl }}/images/autonomicladyland/chuck-folders.png" alt="" title="chuck folders"/>
</div>
 
 There's a few interesting things going on here. Starting with _initialize.ck_ which is the 'Main' file for our ChucK project. Here it is:
 
 ```javascript
 // initialize.ck
 
 Machine.add(me.dir() + "/events/hrmEvent.ck");
 Machine.add(me.dir() + "/events/eventBus.ck");
 
 Machine.add(me.dir() + "/BPM.ck");
 Machine.add(me.dir() + "/score.ck");
 Machine.add(me.dir() + "/osc.ck");
 ```

It's pretty simple - it just loads all the other required files. Note that _hrmEvent.ck_ has to load before _eventBus.ck_ to satisfy some dependency requirements. Let's take a look at _eventBus.ck_:

```javascript
//eventBus.ck

public class eventBus{
    static HrmEvent @ hrmEvent;
}
new HrmEvent @=> eventBus.hrmEvent;
``` 
This will get filled with all the events needed to control the app but at the moment there is just one event - the hrm (heart rate monitor) event. Created like this it allows us to broadcast the event across the entire app so any of the other classes/shreds can register to receive notifications when this event fires. 

And HrmEvent looks like this:

```javascript
//hrmEvent.ck

public class HrmEvent extends Event
{
    float val;
}
```
It simply takes a single float value (though of course it could take many more).

Now, we want to fire this event every time we get an incoming OSC message from (in this case) the heart rate monitor. This gets set up in osc.ck:

```javascript
//osc.ck
// create our OSC receiver
OscRecv oscin;

3001 => oscin.port;

// start listening (launch thread)
oscin.listen();

// ..or listen to all messages
//oscin.listenAll();

// create an address in the receiver 
// and store it in a new variable.
oscin.event("/hello,f") @=> OscEvent HelloEvent; 

while ( true ){
    
    HelloEvent => now;
    
    while ( HelloEvent.nextMsg() != 0 ){
        
        //chuck the value from the osc message into the hrmEvent
        HelloEvent.getFloat() => eventBus.hrmEvent.val;
        
        //this event will be broadcast to all shreds that are listening
        eventBus.hrmEvent.broadcast();
    }
    
}
```

So the value from the incoming OSC message will be broadcast using the static hrmEvent which can be listened to by any shred that is interested. In this way we can receive a single value and have it affect multiple properties of multiple instruments, effects etc.
 
- - -
**12.08.19**

Senheiser Ambeo headset arrives and I get a chance to check out making immersive 3D audio recordings. Quality is excellent - so good that I am fooled into thinking the recorded road traffic noise is live. I'm playing the recording through a pair of Beats Studio headphones with noise cancelling turned on. Fidelity is amazing. This is really going to add a nice real-world element to the piece.

{% include soundcloudplayer.html id=page.soundcloud %}

Here's a recording I made at Second Home in London Fields. The recording was made in the main co-working space which backs onto the creche - the baby screams come from behind a closed door. Listen to this on a pair of headphones - preferably noise-cancelling.